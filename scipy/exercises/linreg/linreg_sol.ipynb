{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609a54aa-c88a-40e5-9f32-ff10645eef75",
   "metadata": {},
   "source": [
    "## Linear Fit with Trial & Error\n",
    "In this exercise we simulate a simple linear fit algorithm. The idea is to vary the slope and find the value that corresponds to the _least square_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d260180d-2132-46a6-8b18-bab235eb2a34",
   "metadata": {},
   "source": [
    "First, we create some fictional data. The y-values linearly depend on x, but with a random perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624e75ec-bb4f-4c75-b9a0-b80434e7f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_points = 10 # Number of data points\n",
    "slope = 2 # Slope of the linear function\n",
    "noise = 1 # Noise amplitude\n",
    "\n",
    "rng = np.random.default_rng() # Random number generator\n",
    "x = np.arange(n_points) # Generate x values\n",
    "y = slope * x + noise * rng.random(n_points) # Generate linear data with noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1b517-096f-4f7f-ae39-92c0495a8964",
   "metadata": {},
   "source": [
    "Make a graph of y vs. x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc652e-e920-4acf-b885-027cf21a8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y, '.')\n",
    "ax.set_title('Linear Data with Noise')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb590d4-0165-4e88-a39b-793e8d672373",
   "metadata": {},
   "source": [
    "Now we assume that we do not know the \"exact\" slope, but we can estimate it to be between 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7c3960-8dd5-47e3-ab81-3d3f345cb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_dev(x, y, m):\n",
    "    diff = y - m * x\n",
    "    sq = diff ** 2\n",
    "    return np.sum(sq)\n",
    "\n",
    "m_min = 1\n",
    "m_max = 3\n",
    "\n",
    "step = 0.001\n",
    "trials = np.arange(m_min, m_max, step)\n",
    "\n",
    "lsq = np.array([sq_dev(x, y, t) for t in trials])\n",
    "\n",
    "min_index = np.argmin(lsq)\n",
    "opt = trials[min_index]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(trials, lsq, '-')\n",
    "ax.plot(opt, lsq[min_index], 'ro', label=f'least square deviation for m = {opt:.3f}')\n",
    "ax.set_title('Square Deviation vs Slope')\n",
    "ax.set_xlabel('slope')\n",
    "ax.set_ylabel('square deviation')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b030a0c6-f637-4a30-ae4d-49f23a92b733",
   "metadata": {},
   "source": [
    "Compare the result to the values obtained with a built-in fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa18f8-c005-468c-be1d-d462de40e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "def f(x, m):\n",
    "    return m * x\n",
    "\n",
    "coeff, pcov = curve_fit(f, x, y)\n",
    "m = coeff[0]\n",
    "\n",
    "print(f'Best slope according to scipy.curve_fit: m = {m:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec44068-822b-4032-8046-0344739d8a11",
   "metadata": {},
   "source": [
    "As long as the steps for the guess are small enough, the result of the (more efficient) curve_fit can be reproduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad89c3f",
   "metadata": {},
   "source": [
    "## Formal derivation\n",
    "Show that the value for the slope that minimises the square deviation is given by\n",
    "$$ m = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc8d0d",
   "metadata": {},
   "source": [
    "For an arbitrary slope $m$ the square deviation is given by\n",
    "$$ \\chi^2 = \\sum_{i=1}^n (m x_i - y_i)^2 $$\n",
    "\n",
    "To minimise this expression, we have to take its derivative with respect to $m$:\n",
    "$$ \\frac{\\textrm{d}\\chi^2}{\\textrm{d}m} = \\sum_{i=1}^n 2 (m x_i - y_i) x_i = \n",
    "2 \\left(m \\sum_{i=1}^n x_i^2 - \\sum_{i=1}^n x_i y_i \\right) $$\n",
    "\n",
    "For a minimum this has to be zero. It follows the expression above for the least square."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0985964e",
   "metadata": {},
   "source": [
    "#### Numerical verification\n",
    "Verify if the formal expression leads to the same result for the optimum slope as the calculations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d98d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sxi2 = np.sum(x**2) # sum of x_i^2\n",
    "sxiyi = np.sum(x*y) # sum of x_i y_i\n",
    "\n",
    "m = sxiyi / sxi2\n",
    "\n",
    "print(f'Best slope according to scipy.curve_fit: m = {m:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb2048",
   "metadata": {},
   "source": [
    "## Error of slope\n",
    "In order to calculate the error of the slope, we can use the general rule for the error propagation of a quantity that depends on the measured values $x_1, x_2, \\dots, x_n$ and $y_1, y_2, \\dots, y_n$:\n",
    "$$ \\Delta m = \\sum_{i=1}^n \\frac{\\partial m}{\\partial x_i} \\Delta x_i + \\sum_{i=1}^n \\frac{\\partial m}{\\partial y_i} \\Delta y_i $$\n",
    "\n",
    "In the following we assume that the uncertainties for all $x$ values are the same, and that all uncertainties for $y$ are the same, i.e. $\\Delta x_i = \\Delta x$ and $\\Delta y_i = \\Delta y$ for all values of $i$.\n",
    "\n",
    "Derive a formal expression for $\\Delta m$ and calculate the numerical value for uncertainties $\\Delta x = 0.1$ and $\\Delta y = 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27def00",
   "metadata": {},
   "source": [
    "The derivatives of $m$ with respect to $x_i$ are (with the product rule)\n",
    "$$ \\dfrac{\\partial m}{\\partial x_i} = \\frac{\\frac{\\partial}{\\partial x_i} \\left( \\sum_{j=1}^n x_j y_j \\right)}{\\sum_{j=1}^n x_j^2}\n",
    "+ \\sum_{j=1}^n x_j y_j \\cdot {\\frac{\\partial}{\\partial x_i} \\left(\\sum_{j=1}^n x_j^2 \\right)^{-1}} $$\n",
    "\n",
    "The first term is just \n",
    "$$ \\frac{y_i}{\\sum_{j=1}^n x_j^2} $$\n",
    "\n",
    "(all other terms in the sum are constant with respect to $x_i$), and the second term can be written as (using the chain rule)\n",
    "$$ \\sum_{j=1}^n x_j y_j \\cdot \\left( - \\left(\\sum_{j=1}^n x_j^2 \\right)^{-2} \\cdot 2 x_i \\right) $$\n",
    "\n",
    "For the derivatives with respect to $y_i$, the numerator is constant, so we find\n",
    "$$ \\dfrac{\\partial m}{\\partial y_i} = \\frac{x_i}{\\sum_{j=1}^n x_j^2} $$\n",
    "\n",
    "Putting everything together, we find\n",
    "\n",
    "$$ \\Delta m = \\left( \\frac{\\sum y_i}{\\sum x_i^2} - 2 \\frac{\\sum x_i \\sum x_i y_i}{\\left( \\sum x_i^2 \\right)^2} \\right) \\Delta x + \\frac{\\sum x_i}{\\sum x_i^2} \\Delta y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29acc17",
   "metadata": {},
   "source": [
    "#### Numerical calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sxi = np.sum(x)\n",
    "syi = np.sum(y)\n",
    "sxi2 = np.sum(x**2)\n",
    "sxiyi = np.sum(x*y)\n",
    "\n",
    "dx = 0.1\n",
    "dy = 0.5\n",
    "\n",
    "dm = syi/sxi2 * dx + sxi/sxi2 * dy - 2*sxiyi * sxi / sxi2**2 * dx\n",
    "\n",
    "print(f'{m:.3f} Â± {dm:.3f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
